{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST classification example\n",
    "\n",
    "This example shows how to build deep learning pipelines in PyPipeline.\n",
    "\n",
    "We create a training and inference pipeline for MNIST classification, using a\n",
    "ResNet classifier and PyTorch-Lightning. PyTorch-Lightning is a nice abstraction layer on top of\n",
    "pure PyTorch, which takes away a lot of the boilerplate code of neural network training.\n",
    "\n",
    "In the inference pipeline, we scale up the ResNet classifier cell for higher throughput.\n",
    "\n",
    "## Model code\n",
    "\n",
    "First, install the required prerequisites:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!pip install torch>=1.6 torchvision>=0.7\n",
    "!pip install pytorch-lightning>=1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torchmetrics.functional import accuracy\n",
    "import torchvision.models.resnet as resnet\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we define the PyTorch Lightning module, which is totally independent of PyPipeline.\n",
    "This code defines the neural network architecture to use, the loss functions, and how the\n",
    "training should look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class ResNetClassifierModule(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, num_classes: int = 10, num_input_channels: int = 1):\n",
    "        super(ResNetClassifierModule, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.num_input_channels = num_input_channels\n",
    "        self.save_hyperparameters()     # saves our __init__ arguments for restoring\n",
    "\n",
    "        # Configure the model\n",
    "        self.model = resnet.resnet18(pretrained=False, num_classes=num_classes)\n",
    "        self.model.conv1 = torch.nn.Conv2d(num_input_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.final_layer = torch.nn.Softmax(dim=1)\n",
    "\n",
    "        # Configure loss function\n",
    "        self.loss_fn = CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = self.model(x)\n",
    "        x = self.final_layer(x)\n",
    "        return x\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.001)\n",
    "\n",
    "    def training_step(self, batch: Tuple[Tensor, Tensor], batch_idx: int):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss_fn(y_hat.squeeze(dim=1), y)\n",
    "        prediction = torch.argmax(y_hat, dim=1)\n",
    "        acc = accuracy(prediction, y)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        self.log(\"train_acc\", acc)\n",
    "        return loss\n",
    "\n",
    "    def evaluate(self, batch, stage=None):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss_fn(y_hat.squeeze(dim=1), y)\n",
    "        prediction = torch.argmax(y_hat, dim=1)\n",
    "        acc = accuracy(prediction, y)\n",
    "        self.log(f\"{stage}_loss\", loss, prog_bar=True)\n",
    "        self.log(f\"{stage}_acc\", acc, prog_bar=True)\n",
    "\n",
    "    def validation_step(self, batch: Tuple[Tensor, Tensor], batch_idx: int):\n",
    "        self.evaluate(batch, \"val\")\n",
    "\n",
    "    def test_step(self, batch: Tuple[Tensor, Tensor], batch_idx: int):\n",
    "        self.evaluate(batch, \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now that we've created the PyTorch Lightning module, we could train or execute it outside of PyPipeline as follows:\n",
    "\n",
    "```\n",
    "classifier = ResNetClassifierModule()\n",
    "\n",
    "# Training:\n",
    "trainer = pl.Trainer(max_epochs=epochs, gpus=num_gpus)\n",
    "trainer.fit(classifier, train_dataloader, val_dataloader)\n",
    "\n",
    "# Inference\n",
    "label = classifier(image)\n",
    "```\n",
    "\n",
    "## Training pipeline\n",
    "\n",
    "To use it in PyPipeline, we have to put it inside a pipeline. We'll build the pipeline as follows:\n",
    "\n",
    "![Training pipeline](mnistclassification-training-pipeline.png)\n",
    "\n",
    "The training pipeline consists of 3 cells:\n",
    "- Two ``DataLoaderSourceCells``, which can be configured with a PyTorch ``DataLoader`` object to load samples\n",
    "  and their corresponding labels. As PyTorch dataloaders load their data in batches,\n",
    "  this cell has an option to remove the batch dimension when the dataloader batch size is set to 1.\n",
    "- A ``ResNetClassifierTrainingsCell``, which trains the whole network in 1 ``pull()`` call.\n",
    "\n",
    "Note that this pipeline uses **advanced control flow**: the pipeline has only to be pulled once to fully train the model.\n",
    "During that pull, the training cell pulls its inputs multiple times in different phases.\n",
    "\n",
    "1. The ``train_image`` and ``train_label`` inputs are pulled until the train source raises a\n",
    "   ``StopIteration`` when its dataloader is empty. This signals the training cell that the validation should begin.\n",
    "2. The ``val_image`` and ``val_label`` inputs are pulled until the validation source raises a ``StopIteration``.\n",
    "   At this moment, one epoch has finished.\n",
    "3. Before starting the next epoch, a reset signal is sent to both data source cells, such that their dataloaders\n",
    "   can be reinitialized (and reshuffled).\n",
    "\n",
    "**PyPipeline provides tools to handle this advanced control flow in the ``pypipeline_lib`` package:**\n",
    "- The ``pypipeline_lib.torch.DataLoaderSourceCell`` is a source cell that can be configured using a PyTorch dataloader.\n",
    "- The ``pypipeline_lib.torch.CellInputDataset`` and ``pypipeline_lib.torch.CellInputDataLoader`` are PyTorch dataset\n",
    "  and dataloader objects that can load their data out of a PyPipeline cell's inputs.\n",
    "\n",
    "They handle the ``StopIteration``, reinitialization and reshuffling behind the scenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "from pypipeline.cell import ASingleCell, ACompositeCell, Pipeline, ScalableCell\n",
    "from pypipeline.cellio import Input, Output, ConfigParameter, InputPort, OutputPort\n",
    "from pypipeline.connection.connection import Connection\n",
    "from pypipeline_lib.torch import CellInputDataLoader, CellInputDataset, DataLoaderSourceCell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class ResNetClassifierTrainingsCell(ASingleCell):\n",
    "\n",
    "    def __init__(self, parent_cell: \"Optional[ACompositeCell]\", name: str):\n",
    "        super(ResNetClassifierTrainingsCell, self).__init__(parent_cell, name)\n",
    "\n",
    "        # Create inputs and outputs\n",
    "        self.input_train_image: Input[Tensor] = Input(self, \"train_image\")\n",
    "        self.input_train_label: Input[Tensor] = Input(self, \"train_label\")\n",
    "        self.input_val_image: Input[Tensor] = Input(self, \"val_image\")\n",
    "        self.input_val_label: Input[Tensor] = Input(self, \"val_label\")\n",
    "        self.output_model_path: Output[Path] = Output(self, \"output_model_path\")\n",
    "\n",
    "        # Create configuration parameters\n",
    "        # The number of processes is how many times the model must be created in parallel. If use_gpu is on,\n",
    "        # then an equal amount of GPUs should be available.\n",
    "        self.config_use_gpu: ConfigParameter[bool] = ConfigParameter(self, \"use_gpu\")\n",
    "        self.config_num_processes: ConfigParameter[int] = ConfigParameter(self, \"num_processes\")\n",
    "        self.config_batch_size: ConfigParameter[int] = ConfigParameter(self, \"batch_size\")\n",
    "        self.config_epochs: ConfigParameter[int] = ConfigParameter(self, \"epochs\")\n",
    "\n",
    "        # Only set after cell deployment\n",
    "        self.classifier: Optional[ResNetClassifierModule] = None\n",
    "\n",
    "    def _on_deploy(self) -> None:\n",
    "        super(ResNetClassifierTrainingsCell, self)._on_deploy()\n",
    "        self.classifier = ResNetClassifierModule(num_classes=10, num_input_channels=1)\n",
    "\n",
    "    def _on_undeploy(self) -> None:\n",
    "        super(ResNetClassifierTrainingsCell, self)._on_undeploy()\n",
    "        self.classifier = None\n",
    "\n",
    "    def _on_pull(self) -> None:\n",
    "        assert self.classifier is not None\n",
    "\n",
    "        # Request the values of our parameters\n",
    "        batch_size = self.config_batch_size.get_value()\n",
    "        use_gpu = self.config_use_gpu.get_value()\n",
    "        num_processes = self.config_num_processes.get_value()\n",
    "        epochs = self.config_epochs.get_value()\n",
    "\n",
    "        # Wrap our image and label inputs in a torch.utils.data.Dataset object.\n",
    "        # PyPipeline has a specific dataset class tailored for this: the CellInputDataset.\n",
    "        train_dataset = CellInputDataset(self.input_train_image, self.input_train_label)\n",
    "        train_dataloader = CellInputDataLoader(train_dataset, batch_size=batch_size)\n",
    "\n",
    "        val_dataset = CellInputDataset(self.input_val_image, self.input_val_label)\n",
    "        val_dataloader = CellInputDataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "        # Start the training\n",
    "        checkpoint_callback = ModelCheckpoint(verbose=False, monitor=\"val_loss\")     # This is very configurable\n",
    "        if use_gpu:\n",
    "            trainer = pl.Trainer(max_epochs=epochs, gpus=num_processes,\n",
    "                                 callbacks=[checkpoint_callback])\n",
    "        else:\n",
    "            trainer = pl.Trainer(max_epochs=epochs, num_processes=num_processes, distributed_backend=\"ddp_cpu\",\n",
    "                                 callbacks=[checkpoint_callback])\n",
    "\n",
    "        trainer.fit(self.classifier, train_dataloader, val_dataloader)\n",
    "\n",
    "        # Make the path to the best model checkpoint available as output of the cell.\n",
    "        best_model_path = Path(checkpoint_callback.best_model_path)\n",
    "        print(f\"Best model path: {best_model_path}\")\n",
    "        self.classifier = self.classifier.load_from_checkpoint(str(best_model_path))\n",
    "        self.output_model_path.set_value(best_model_path)\n",
    "\n",
    "    def supports_scaling(self) -> bool:\n",
    "        # This cell doesn't support parallelizing with PyPipeline, as the training\n",
    "        # process is stateful.\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "As we have defined the PyPipeline cells for our classifier, we can now create a training pipeline for it.\n",
    "This pipeline will also contain source cells (and possibly some other cells as well, ex. preprocessing),\n",
    "which are needed to feed our classifier cell with data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class ResNetClassifierTrainingPipeline(Pipeline):\n",
    "\n",
    "    def __init__(self, parent_cell: \"Optional[ACompositeCell]\", name: str):\n",
    "        super(ResNetClassifierTrainingPipeline, self).__init__(parent_cell, name)\n",
    "\n",
    "        # Create the cells\n",
    "        self.train_source: DataLoaderSourceCell[Tensor, Tensor] = \\\n",
    "            DataLoaderSourceCell(self, \"train_source\")\n",
    "        self.val_source: DataLoaderSourceCell[Tensor, Tensor] = \\\n",
    "            DataLoaderSourceCell(self, \"val_source\")\n",
    "        self.classifier = ResNetClassifierTrainingsCell(self, \"classifier_training_cell\")\n",
    "\n",
    "        # Create the connections\n",
    "        Connection(self.train_source.output_sample, self.classifier.input_train_image)\n",
    "        Connection(self.train_source.output_label, self.classifier.input_train_label)\n",
    "\n",
    "        Connection(self.val_source.output_sample, self.classifier.input_val_image)\n",
    "        Connection(self.val_source.output_label, self.classifier.input_val_label)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We instantiate our training pipelines.\n",
    "Some cells need some configuration, like the classifier cell (num_epochs, ...) and\n",
    "the source cells (which dataloaders to use, ...).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms.functional import to_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Prepare Dataset and Dataloader to provide to our dataloader source cells.\n",
    "mnist_train = MNIST(\"/home/johannes/data/mnist/\", train=True, download=True, transform=to_tensor)\n",
    "mnist_train, mnist_val = random_split(mnist_train, [55000, 5000])       # type: ignore\n",
    "mnist_test = MNIST(\"/home/johannes/data/mnist/\", train=False, download=True, transform=to_tensor)\n",
    "\n",
    "# During training, we want our data to flow one by one through the pipeline, so we use batch size one\n",
    "mnist_train_loader = DataLoader(mnist_train, batch_size=1, num_workers=4, shuffle=True)\n",
    "mnist_val_loader = DataLoader(mnist_val, batch_size=1, num_workers=4, shuffle=True)\n",
    "# But we'll do inference on batches of 4:\n",
    "mnist_test_loader = DataLoader(mnist_test, batch_size=4, num_workers=4, shuffle=True)\n",
    "\n",
    "# Training pipeline\n",
    "# =================\n",
    "# Creation\n",
    "training_pipeline = ResNetClassifierTrainingPipeline(None, \"mnist_training_pipeline\")\n",
    "\n",
    "# Configuration\n",
    "training_pipeline.classifier.config_use_gpu.set_value(True)\n",
    "training_pipeline.classifier.config_num_processes.set_value(1)\n",
    "training_pipeline.classifier.config_batch_size.set_value(100)\n",
    "training_pipeline.classifier.config_epochs.set_value(20)\n",
    "\n",
    "training_pipeline.train_source.config_dataloader.set_value(mnist_train_loader)\n",
    "training_pipeline.train_source.config_remove_batch_dimension.set_value(True)\n",
    "training_pipeline.val_source.config_dataloader.set_value(mnist_val_loader)\n",
    "training_pipeline.val_source.config_remove_batch_dimension.set_value(True)\n",
    "\n",
    "# Train our model\n",
    "training_pipeline.deploy()\n",
    "training_pipeline.pull()\n",
    "best_checkpoint_path = training_pipeline.classifier.output_model_path.get_value()\n",
    "training_pipeline.undeploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference pipeline\n",
    "\n",
    "To use it in PyPipeline, we have to put it inside a pipeline. We'll create the following inference\n",
    "pipeline:\n",
    "\n",
    "![Inference pipeline](mnistclassification-inference-pipeline.png)\n",
    "\n",
    "This example shows batch-wise inference with batches of size 4. Note how the ``mnist_test_loader`` was\n",
    "defined with batch size 4, and how the ``DataLoaderSourceCell.remove_batch_dimension`` configuration parameter is set\n",
    "to False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class ResNetClassifierCell(ASingleCell):\n",
    "\n",
    "    def __init__(self, parent_cell: \"Optional[ACompositeCell]\", name: str):\n",
    "        super(ResNetClassifierCell, self).__init__(parent_cell, name)\n",
    "\n",
    "        # Create inputs and outputs\n",
    "        self.input_images: Input[Tensor] = Input(self, \"images\")\n",
    "        self.output_labels: Output[Tensor] = Output(self, \"labels\")\n",
    "\n",
    "        # Create configuration parameters\n",
    "        self.config_use_gpu: ConfigParameter[bool] = ConfigParameter(self, \"use_gpu\")\n",
    "        self.config_checkpoint: ConfigParameter[Path] = ConfigParameter(self, \"checkpoint_to_load\")\n",
    "\n",
    "        # Only set after cell deployment\n",
    "        self.classifier: Optional[ResNetClassifierModule] = None\n",
    "\n",
    "    def _on_deploy(self) -> None:\n",
    "        super(ResNetClassifierCell, self)._on_deploy()\n",
    "        ckpt_path: Path = self.config_checkpoint.get_value()\n",
    "        self.classifier = ResNetClassifierModule.load_from_checkpoint(str(ckpt_path))\n",
    "        self.classifier.eval()\n",
    "\n",
    "    def _on_undeploy(self) -> None:\n",
    "        super(ResNetClassifierCell, self)._on_undeploy()\n",
    "        self.classifier = None\n",
    "\n",
    "    def _on_pull(self) -> None:\n",
    "        assert self.classifier is not None\n",
    "\n",
    "        # Pull the inputs and set the outputs\n",
    "        images: Tensor = self.input_images.pull()         # Input: shape (batch, channels, x, y), torch.float32, range [0, 1]\n",
    "        labels = self.classifier(images)  # shape (batch, classes,), torch.float32, range [0, 1]\n",
    "        labels = labels.cpu().detach()\n",
    "        self.output_labels.set_value(labels)\n",
    "\n",
    "    def supports_scaling(self) -> bool:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define an inference pipeline for our classifier cell.\n",
    "Next to the necessary source cells, we also add a ScalableCell, which will\n",
    "allow our classifier cell to be scaled up as much as we need.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class ResNetClassifierScalableCell(ScalableCell):\n",
    "\n",
    "    def __init__(self, parent_cell: \"Optional[ACompositeCell]\", name: str):\n",
    "        super(ResNetClassifierScalableCell, self).__init__(parent_cell, name)\n",
    "\n",
    "        # Create the inputs and outputs\n",
    "        self.input_images: InputPort[Tensor] = InputPort(self, \"images\")\n",
    "        self.output_labels: OutputPort[Tensor] = OutputPort(self, \"labels\")\n",
    "\n",
    "        # Create the cells\n",
    "        self.classifier = ResNetClassifierCell(self, \"classifier_cell\", )\n",
    "\n",
    "        # Create the connections\n",
    "        Connection(self.input_images, self.classifier.input_images)\n",
    "        Connection(self.classifier.output_labels, self.output_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class ResNetClassifierInferencePipeline(Pipeline):\n",
    "\n",
    "    def __init__(self, parent_cell: \"Optional[ACompositeCell]\", name: str):\n",
    "        super(ResNetClassifierInferencePipeline, self).__init__(parent_cell, name)\n",
    "\n",
    "        # Create the cells\n",
    "        # TODO also make a sink cell to store the result?\n",
    "        # In the inference pipeline, we want to do batch predictions, so don't remove the batch dim:\n",
    "        self.source: DataLoaderSourceCell[Tensor, Tensor] = DataLoaderSourceCell(self, \"source\")\n",
    "        self.classifier_scalable = ResNetClassifierScalableCell(self, \"classifier_scalable\")\n",
    "\n",
    "        # Create the connections\n",
    "        Connection(self.source.output_sample, self.classifier_scalable.input_images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Inference pipeline\n",
    "# ==================\n",
    "# Now that we have trained our classifier, we can use it in an inference pipeline which supports upscaling.\n",
    "inference_pipeline = ResNetClassifierInferencePipeline(None, \"mnist_inference_pipeline\")\n",
    "\n",
    "# Configuration\n",
    "inference_pipeline.classifier_scalable.classifier.config_use_gpu.set_value(True)\n",
    "inference_pipeline.classifier_scalable.classifier.config_checkpoint.set_value(best_checkpoint_path)\n",
    "\n",
    "inference_pipeline.source.config_dataloader.set_value(mnist_test_loader)\n",
    "inference_pipeline.source.config_remove_batch_dimension.set_value(False)\n",
    "\n",
    "# Scale up our scalable cell to 2 parallel instances\n",
    "inference_pipeline.classifier_scalable.scale_up(times=2)\n",
    "inference_pipeline.deploy()\n",
    "\n",
    "print(\"Start inference\")\n",
    "for i in range(inference_pipeline.get_nb_available_pulls()):\n",
    "    inference_pipeline.pull()\n",
    "\n",
    "    # We could add a sink cell that writes the labels to disk or visualizes them together with the input image,\n",
    "    # but for now we just print them out:\n",
    "    labels = inference_pipeline.classifier_scalable.output_labels.get_value()\n",
    "    print(f\"Classification probabilities: {labels}\")\n",
    "\n",
    "inference_pipeline.undeploy()\n",
    "print(\"Finished\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}